{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "Consider the neural network considered in the first question of the theoretical component of the practical class, with number of units: 4,4,3,3.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1SHUgdosKp6AX8rRAACCZ5nb4kUXreI3g)\n",
    "\n",
    "Assume all units, except the ones in the output layer, use the hyperbolic tangent activation function. \n",
    "\n",
    "Consider the following training example:\n",
    "\n",
    "$\\mathbf{x} =\\begin{bmatrix} 1, 0, 1, 0 \\end{bmatrix}^\\intercal $,   $\\mathbf{y} =\\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "❓ Using the squared error loss do a stochastic gradient descent update, initializing all connection weights and biases to 0.1 and a  learning rate η = 0.1:\n",
    "\n",
    "1. Perform the forward pass\n",
    "2. Compute the loss\n",
    "3. Compute gradients with backpropagation\n",
    "4. Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1) (3, 1) (4, 4) (3, 4) (3, 3) (4, 1) (3, 1) (3, 1)\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[1, 0, 1, 0]]).T\n",
    "labels = np.array([[0, 1, 0]]).T\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "\n",
    "# Initialize weights with correct shapes \n",
    "w = [np.full((units[i+1], units[i]), 0.1) for i in range(0,len(units)-1)]\n",
    "learning_rate = 0.1\n",
    "bias = [np.full((units[i+1], 1), 0.1) for i in range(0,len(units)-1)]\n",
    "print(inputs.shape, labels.shape, w[0].shape, w[1].shape, w[2].shape, bias[0].shape, bias[1].shape, bias[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "layer = [[] for i in range(0,len(units))]\n",
    "layer[0] = inputs\n",
    "for i in range(0, len(units)-1):\n",
    "    z = w[i] @ layer[i] + bias[i]\n",
    "    if i != len(units)-2:\n",
    "        layer[i+1] = np.tanh(z)\n",
    "    else:\n",
    "        layer[i+1] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.37636378397755565, array([[ 0.16396106, -0.83603894,  0.16396106]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "Loss = 1/2*np.sum((layer[-1] - labels)**2)\n",
    "d_Loss = layer[-1] - labels\n",
    "Loss, d_Loss.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "grad_z = d_Loss\n",
    "for i in range(len(w)-1, -1, -1):\n",
    "    grad_h = w[i].T @ grad_z\n",
    "    w[i] = w[i] - learning_rate * (grad_z @ layer[i].T) \n",
    "    bias[i] = bias[i] - learning_rate * (grad_z) \n",
    "    grad_z = grad_h * ((1 - layer[i]**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Let's say we were using the same training example but with the following changes:\n",
    "- The output units have a softmax activation function\n",
    "- The error function is cross-entropy\n",
    "\n",
    "Keeping the same initializations and learning rate, adjust your computations to the new changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We need only to change:  \n",
    "- the output, *i.e.*, $\\hat{y} = softmax(z_3)$ instead of $\\hat{y} = z_3$\n",
    "- the loss computation to $L = -y.log(\\hat{y})$\n",
    "- the gradient of the loss with respect to $z_3$: $\\frac{dL}{dz_3}$\n",
    "\n",
    "All other steps remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[1, 0, 1, 0]]).T\n",
    "labels = np.array([[0, 1, 0]]).T\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "\n",
    "# Initialize weights with correct shapes \n",
    "w = [np.full((units[i+1], units[i]), 0.1) for i in range(0,len(units)-1)]\n",
    "learning_rate = 0.1\n",
    "bias = [np.full((units[i+1], 1), 0.1) for i in range(0,len(units)-1)]\n",
    "\n",
    "\n",
    "# Forward Pass\n",
    "layer = [[] for i in range(0,len(units))]\n",
    "layer[0] = inputs\n",
    "for i in range(0, len(units)-1):\n",
    "    z = w[i] @ layer[i] + bias[i]\n",
    "    if i != len(units)-2:\n",
    "        layer[i+1] = np.tanh(z)\n",
    "    else:\n",
    "        layer[i+1] = np.exp(z)/np.sum(np.exp(z))\n",
    "        \n",
    "        \n",
    "Loss = -labels.T @ np.log(layer[-1])\n",
    "grad_z = layer[-1] - labels\n",
    "for i in range(len(w)-1, -1, -1):\n",
    "    grad_h = w[i].T @ grad_z\n",
    "    w[i] = w[i] - learning_rate * (grad_z @ layer[i].T) \n",
    "    bias[i] = bias[i] - learning_rate * (grad_z) \n",
    "    grad_z = grad_h * ((1 - layer[i]**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete functions `forward`, `compute_loss`, `backpropagation` and `update_weights` generalized to perform the same computations as before, but for any MLP architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "x: single observation of shape (n,)\n",
    "weights: list of weight matrices [W1, W2, ...]\n",
    "biases: list of biases matrices [b1, b2, ...]\n",
    "\n",
    "y: final output\n",
    "hiddens: list of computed hidden layers [h1, h2, ...]\n",
    "'''\n",
    "def forward(x, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    g = np.tanh\n",
    "    hiddens = []\n",
    "    for i in range(num_layers):\n",
    "        h = x if i == 0 else hiddens[i-1]\n",
    "        z = weights[i] @ h + biases[i]\n",
    "        # compute hidden layer\n",
    "        if i != num_layers-1:\n",
    "            hiddens.append(g(z))\n",
    "        \n",
    "        #compute output\n",
    "        else:\n",
    "            #output = np.exp(z)/np.sum(np.exp(z))\n",
    "            output = z\n",
    "\n",
    "    return output, hiddens\n",
    "\n",
    "def compute_loss(output, y):\n",
    "    probs = np.exp(output) / np.sum(np.exp(output))\n",
    "    loss = -y @ np.log(probs)\n",
    "    return loss   \n",
    "\n",
    "\n",
    "def backward(x, y, output, hiddens, weights):\n",
    "    num_layers = len(weights)\n",
    "    g = np.tanh\n",
    "    z = output\n",
    "\n",
    "    probs = np.exp(output) / np.sum(np.exp(output))\n",
    "    grad_z = probs - y  \n",
    "    \n",
    "    grad_weights = []\n",
    "    grad_biases = []\n",
    "    \n",
    "    # Backpropagate gradient computations \n",
    "    for i in range(num_layers-1, -1, -1):\n",
    "        h = x if i == 0 else hiddens[i-1]\n",
    "        # Gradient of hidden parameters.\n",
    "        grad_weights.append(grad_z[:, None]@ h[:, None].T)\n",
    "        grad_biases.append(grad_z)\n",
    "        grad_h = weights[i].T @ grad_z\n",
    "        grad_z = grad_h * (1 - h**2)\n",
    "    # Making gradient vectors have the correct order\n",
    "    grad_weights.reverse()\n",
    "    grad_biases.reverse()\n",
    "    return grad_weights, grad_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Now we will use the MLP on real data to classify handwritten digits.\n",
    "\n",
    "Data is loaded, split into train and test sets and target is one-hot encoded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_digits()\n",
    "\n",
    "inputs = data.data  \n",
    "labels = data.target  \n",
    "n, p = np.shape(inputs)\n",
    "n_classes = len(np.unique(labels))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode labels as one-hot vectors.\n",
    "one_hot = np.zeros((np.size(y_train, 0), n_classes))\n",
    "for i in range(np.size(y_train, 0)):\n",
    "    one_hot[i, y_train[i]] = 1\n",
    "y_train_ohe = one_hot\n",
    "one_hot = np.zeros((np.size(y_test, 0), n_classes))\n",
    "for i in range(np.size(y_test, 0)):\n",
    "    one_hot[i, y_test[i]] = 1\n",
    "y_test_ohe = one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_train_epoch` using your previously defined functions to compute one epoch of training using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Outputs:\n",
    "    - weights: list of updated weights\n",
    "    - biases: list of updated \n",
    "    - loss: scalar of total loss (sum for all observations)\n",
    "\n",
    "'''\n",
    "\n",
    "def MLP_train_epoch(inputs, labels, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    total_loss = 0\n",
    "    \n",
    "    # For each observation and target\n",
    "    for x, y in zip(inputs, labels):\n",
    "        # Compute forward pass\n",
    "        output, hiddens = forward(x, weights, biases)\n",
    "        # Compute Loss and update total loss\n",
    "        loss = compute_loss(output, y)\n",
    "        total_loss += loss\n",
    "        # Compute backpropagation\n",
    "        grad_weights, grad_biases = backward(x, y, output, hiddens, weights)\n",
    "        # Update weights\n",
    "        for i in range(num_layers):\n",
    "            weights[i] -= learning_rate * grad_weights[i]\n",
    "            biases[i] -= learning_rate * grad_biases[i]\n",
    "         \n",
    "    return weights, biases, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a MLP with a single hidden layer of 50 units and a learning rate of $0.001$. \n",
    "\n",
    "❓ Run 100 epochs of your MLP. Save the loss at each epoch in a list and plot the loss evolution after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f11643986a0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize weights\n",
    "units = [p, 50, n_classes]\n",
    "w = [.1 * np.random.randn(units[i+1], units[i]) for i in range(0,len(units)-1)]\n",
    "b = [.1 * np.random.randn(units[i+1]) for i in range(0,len(units)-1)]\n",
    "\n",
    "# Empty loss list\n",
    "loss = []\n",
    "# Learning rate.\n",
    "learning_rate = 0.001    \n",
    "# Run epochs and append loss to list\n",
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    w, b, total_loss = MLP_train_epoch(X_train, y_train_ohe, w, b)\n",
    "    loss.append(total_loss)\n",
    "# Plot loss evolution\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_predict` to get array of predictions from your trained MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_predict(inputs, weights, biases):\n",
    "    predicted_labels = []\n",
    "    for x in inputs:\n",
    "        # Compute forward pass\n",
    "        y, _ = forward(x, weights, biases)\n",
    "        y = np.argmax(y)\n",
    "        predicted_labels.append(y)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Compute the accuracy on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:  1.0\n",
      "Accuracy on test set:  0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "labels_train = MLP_predict(X_train, w, b)\n",
    "acc_train = np.mean(labels_train == y_train)\n",
    "\n",
    "labels_test = MLP_predict(X_test, w, b)\n",
    "acc_test = np.mean(labels_test == y_test)\n",
    "print(\"Accuracy on training set: \", acc_train)\n",
    "print(\"Accuracy on test set: \", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our results with Sklearn's implementation of the MLP. Compare their accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993041057759221\n",
      "0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50),\n",
    "                    activation='tanh',\n",
    "                    solver='sgd',\n",
    "                    learning_rate='constant',\n",
    "                    learning_rate_init=0.001,\n",
    "                    nesterovs_momentum=False,\n",
    "                    random_state=1,\n",
    "                    max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
