{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "Consider the neural network considered in the first question of the theoretical component of the practical class, with number of units: 4,4,3,3.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1SHUgdosKp6AX8rRAACCZ5nb4kUXreI3g)\n",
    "\n",
    "Assume all units, except the ones in the output layer, use the hyperbolic tangent activation function. \n",
    "\n",
    "Consider the following training example:\n",
    "\n",
    "$\\mathbf{x} =\\begin{bmatrix} 1, 0, 1, 0 \\end{bmatrix}^\\intercal $,   $\\mathbf{y} =\\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "❓ Using the squared error loss do a stochastic gradient descent update, initializing all connection weights and biases to 0.1 and a  learning rate η = 0.1:\n",
    "\n",
    "1. Perform the forward pass\n",
    "2. Compute the loss\n",
    "3. Compute gradients with backpropagation\n",
    "4. Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1) (3, 1) (4, 4) (3, 4) (3, 3) (4, 1) (3, 1) (3, 1)\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[1, 0, 1, 0]]).T\n",
    "labels = np.array([[0, 1, 0]]).T\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "\n",
    "# Initialize weights with correct shapes \n",
    "w = [np.full((units[i+1], units[i]), 0.1) for i in range(0,len(units)-1)]\n",
    "learning_rate = 0.1\n",
    "bias = [np.full((units[i+1], 1), 0.1) for i in range(0,len(units)-1)]\n",
    "print(inputs.shape, labels.shape, w[0].shape, w[1].shape, w[2].shape, bias[0].shape, bias[1].shape, bias[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "layer = [[] for i in range(0,len(units))]\n",
    "layer[0] = inputs\n",
    "for i in range(0, len(units)-1):\n",
    "    z = w[i] @ layer[i] + bias[i]\n",
    "    if i != len(units)-2:\n",
    "        layer[i+1] = np.tanh(z)\n",
    "    else:\n",
    "        layer[i+1] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.37636378397755565, array([[ 0.16396106, -0.83603894,  0.16396106]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "Loss = 1/2*np.sum((layer[-1] - labels)**2)\n",
    "d_Loss = layer[-1] - labels\n",
    "Loss, d_Loss.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "grad_z = d_Loss\n",
    "for i in range(len(w)-1, -1, -1):\n",
    "    grad_h = w[i].T @ grad_z\n",
    "    w[i] = w[i] - learning_rate * (grad_z @ layer[i].T) \n",
    "    bias[i] = bias[i] - learning_rate * (grad_z) \n",
    "    grad_z = grad_h * ((1 - layer[i]**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Let's say we were using the same training example but with the following changes:\n",
    "- The output units have a softmax activation function\n",
    "- The error function is cross-entropy\n",
    "\n",
    "Keeping the same initializations and learning rate, adjust your computations to the new changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We need only to change:  \n",
    "- the output, *i.e.*, $\\hat{y} = softmax(z_3)$ instead of $\\hat{y} = z_3$\n",
    "- the loss computation to $L = -y.log(\\hat{y})$\n",
    "- the gradient of the loss with respect to $z_3$: $\\frac{dL}{dz_3}$\n",
    "\n",
    "All other steps remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[1, 0, 1, 0]]).T\n",
    "labels = np.array([[0, 1, 0]]).T\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "\n",
    "# Initialize weights with correct shapes \n",
    "w = [np.full((units[i+1], units[i]), 0.1) for i in range(0,len(units)-1)]\n",
    "learning_rate = 0.1\n",
    "bias = [np.full((units[i+1], 1), 0.1) for i in range(0,len(units)-1)]\n",
    "\n",
    "\n",
    "# Forward Pass\n",
    "layer = [[] for i in range(0,len(units))]\n",
    "layer[0] = inputs\n",
    "for i in range(0, len(units)-1):\n",
    "    z = w[i] @ layer[i] + bias[i]\n",
    "    if i != len(units)-2:\n",
    "        layer[i+1] = np.tanh(z)\n",
    "    else:\n",
    "        layer[i+1] = np.exp(z)/np.sum(np.exp(z))\n",
    "        \n",
    "        \n",
    "Loss = -labels.T @ np.log(layer[-1])\n",
    "grad_z = layer[-1] - labels\n",
    "for i in range(len(w)-1, -1, -1):\n",
    "    grad_h = w[i].T @ grad_z\n",
    "    w[i] = w[i] - learning_rate * (grad_z @ layer[i].T) \n",
    "    bias[i] = bias[i] - learning_rate * (grad_z) \n",
    "    grad_z = grad_h * ((1 - layer[i]**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete functions `forward`, `compute_loss`, `backpropagation` and `update_weights` generalized to perform the same computations as before, but for any MLP architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x: single observation of shape (n,)\n",
    "weights: list of weight matrices [W1, W2, ...]\n",
    "biases: list of biases matrices [b1, b2, ...]\n",
    "\n",
    "y: final output\n",
    "hiddens: list of computed hidden layers [h1, h2, ...]\n",
    "'''\n",
    "def forward(x, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    g = np.tanh\n",
    "    hiddens = []\n",
    "    for i in range(num_layers):\n",
    "        h = x if i == 0 else hiddens[i-1]\n",
    "        z = weights[i] @ h + biases[i]\n",
    "        # compute hidden layer\n",
    "        if i != num_layers-1:\n",
    "            hiddens.append(g(z))\n",
    "        \n",
    "        #compute output\n",
    "        else:\n",
    "            #output = np.exp(z)/np.sum(np.exp(z))\n",
    "            output = z\n",
    "\n",
    "    return output, hiddens\n",
    "\n",
    "def compute_loss(output, y):\n",
    "    loss = -y @ np.log(output)\n",
    "    return loss   \n",
    "\n",
    "#(4,) (3) (3) (4 ou 3) (4,4 ou 3,4 ou 3,3)\n",
    "def backward(x, y, output, hiddens, weights):\n",
    "    num_layers = len(weights)\n",
    "    g = np.tanh\n",
    "    z = output\n",
    "\n",
    "    probs = np.exp(output) / np.sum(np.exp(output))\n",
    "    grad_z = probs - y  \n",
    "    \n",
    "    grad_weights = []\n",
    "    grad_biases = []\n",
    "    \n",
    "    # Backpropagate gradient computations \n",
    "    for i in range(num_layers-1, -1, -1):\n",
    "        h = x if i == 0 else hiddens[i-1]\n",
    "        # Gradient of hidden parameters.\n",
    "        grad_weights.append(np.expand_dims(grad_z,axis=1) @ np.expand_dims(h,axis=1).T)\n",
    "        grad_biases.append(grad_z)\n",
    "        grad_h = weights[i].T @ grad_z\n",
    "        grad_z = grad_h * (1 - h**2)\n",
    "    # Making gradient vectors have the correct order\n",
    "    grad_weights.reverse()\n",
    "    grad_biases.reverse()\n",
    "    return grad_weights, grad_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Now we will use the MLP on real data to classify handwritten digits.\n",
    "\n",
    "Data is loaded, split into train and test sets and target is one-hot encoded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_digits()\n",
    "\n",
    "inputs = data.data  \n",
    "labels = data.target  \n",
    "n, p = np.shape(inputs)\n",
    "n_classes = len(np.unique(labels))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode labels as one-hot vectors.\n",
    "one_hot = np.zeros((np.size(y_train, 0), n_classes))\n",
    "for i in range(np.size(y_train, 0)):\n",
    "    one_hot[i, y_train[i]] = 1\n",
    "y_train_ohe = one_hot\n",
    "one_hot = np.zeros((np.size(y_test, 0), n_classes))\n",
    "for i in range(np.size(y_test, 0)):\n",
    "    one_hot[i, y_test[i]] = 1\n",
    "y_test_ohe = one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_train_epoch` using your previously defined functions to compute one epoch of training using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Outputs:\n",
    "    - weights: list of updated weights\n",
    "    - biases: list of updated \n",
    "    - loss: scalar of total loss (sum for all observations)\n",
    "\n",
    "'''\n",
    "\n",
    "def MLP_train_epoch(inputs, labels, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    total_loss = 0\n",
    "    \n",
    "    # For each observation and target\n",
    "    for x, y in zip(inputs, labels):\n",
    "        # Compute forward pass\n",
    "        output, hiddens = forward(x, weights, biases)\n",
    "        # Compute Loss and update total loss\n",
    "        compute_loss(output, y)\n",
    "        # Compute backpropagation\n",
    "        grad_weights, grad_biases = backward(x, y, output, hiddens, weights)\n",
    "        # Update weights\n",
    "        for i in range(num_layers):\n",
    "            weights[i] -= learning_rate * grad_weights[i]\n",
    "            biases[i] -= learning_rate * grad_biases[i]\n",
    "         \n",
    "    return weights, biases, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a MLP with a single hidden layer of 50 units and a learning rate of $0.001$. \n",
    "\n",
    "❓ Run 100 epochs of your MLP. Save the loss at each epoch in a list and plot the loss evolution after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2d0ba85370>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize weights\n",
    "units = [p, 50, n_classes]\n",
    "w = [np.full((units[i+1], units[i]), 0.1) for i in range(0,len(units)-1)]\n",
    "b = [np.full((units[i+1]), 0.1) for i in range(0,len(units)-1)]\n",
    "#np.random.randn\n",
    "\n",
    "# Empty loss list\n",
    "loss = []\n",
    "# Learning rate.\n",
    "learning_rate = 0.001    \n",
    "# Run epochs and append loss to list\n",
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    w, b, total_loss = MLP_train_epoch(X_train, y_train_ohe, w, b)\n",
    "    loss.append(total_loss)\n",
    "# Plot loss evolution\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_predict` to get array of predictions from your trained MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_predict(inputs, weights, biases):\n",
    "    predicted_labels = []\n",
    "    for x in inputs:\n",
    "        # Compute forward pass and get the class with the highest probability\n",
    "        pass\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Compute the accuracy on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our results with Sklearn's implementation of the MLP. Compare their accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50),\n",
    "                    activation='tanh',\n",
    "                    solver='sgd',\n",
    "                    learning_rate='constant',\n",
    "                    learning_rate_init=0.001,\n",
    "                    nesterovs_momentum=False,\n",
    "                    random_state=1,\n",
    "                    max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
