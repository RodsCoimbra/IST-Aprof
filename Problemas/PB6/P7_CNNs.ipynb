{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jlGXqM_t452"
      },
      "source": [
        "# P7 - Convolutional Neural Networks (CNNs)\n",
        "We have now learned about the Perceptron, Linear and logistic regression, Multi-layer perceptron and backpropagation, Auto-encoders.\n",
        "\n",
        "In this pratical session about Convolutional Neural Networks (CNNs) we will use the MNIST datasets.\n",
        "\n",
        "First, we will obtain baselines using a Logistic Regression and a Feed-forward Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITJR4snhxdT0"
      },
      "source": [
        "## 0.0 - Imports\n",
        "We will need to import some libraries to be used in this session. Libraries include data visualizers ([matplotlib](https://matplotlib.org/)), neural network package ([torch](https://pytorch.org/)), and other helper packages for data handling ([sklearn](https://scikit-learn.org/), [numpy](https://numpy.org/))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MWGjU3tDw4bD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-od7M6WMN0N"
      },
      "source": [
        "Then, other variable definitions are needed to be set. This includes the size of the dataset we will use, and the configuration of the GPU to be activated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECqewHJ0MM62",
        "outputId": "f5749bd9-87e6-483b-8d1e-220e4aa3c0d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Configure Device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odY0Ng9yycgr"
      },
      "source": [
        "### 0.1 - Create Dataloaders\n",
        "#### MNIST dataset\n",
        "Using torchvision we can easily download and use the MNIST dataset to create our train and validation dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "snFv-Hu-zRnW"
      },
      "outputs": [],
      "source": [
        "# Define tranform - Convert data to tensor and normalize using dataset mean and std\n",
        "# mean and std are computed offline using the training dataset\n",
        "# tranforms.Normalize expects a value of mean and std per image channel\n",
        "mnist_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# Download and create MNIST train and validation dataloaders\n",
        "mnist_train_dataset = datasets.MNIST('../data', download=True, train=True, transform=mnist_transform)\n",
        "mnist_val_dataset = datasets.MNIST('../data', download=True, train=False, transform=mnist_transform)\n",
        "mnist_train_dataloader = DataLoader(mnist_train_dataset, batch_size=64, shuffle=True)\n",
        "mnist_val_dataloader = DataLoader(mnist_val_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# MNIST Dataloaders to get data into numpy for Logistic Regression\n",
        "mnist_train_dataloader_numpy = DataLoader(mnist_train_dataset, batch_size=len(mnist_train_dataset))\n",
        "mnist_val_dataloader_numpy = DataLoader(mnist_val_dataset, batch_size=len(mnist_val_dataset))\n",
        "X_y_train = next(iter(mnist_train_dataloader_numpy))\n",
        "X_y_val = next(iter(mnist_val_dataloader_numpy))\n",
        "X_train = X_y_train[0].numpy()\n",
        "y_train = X_y_train[1].numpy()\n",
        "X_val = X_y_val[0].numpy()\n",
        "y_val = X_y_val[1].numpy()\n",
        "\n",
        "dataloaders = dict(train=mnist_train_dataloader, val=mnist_val_dataloader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJANJSJ0hsqd"
      },
      "source": [
        "We can check the MNIST dataset properties such as:\n",
        "\n",
        "- shape of train and validation datasets - \\[number of samples, width, height\\]\n",
        "- number of input feature on the flattened/reshaped input for Logistic Regression or MLP\n",
        "- shape of train and validation batches - \\[batch size, number of channels, width, height\\]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuluBqnnCbn5",
        "outputId": "ae3765db-a0b9-4c4f-e5bd-9f50eed9f991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets shapes: {'train': torch.Size([60000, 28, 28]), 'val': torch.Size([10000, 28, 28])}\n",
            "N input features: 784 Output classes: 10\n",
            "Train batch: torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
            "Val batch: torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# get batch to extract properties and plot example images\n",
        "# next(enumerator(dataloader)) -> creates an iterator of the dataloader and gets the next batch√ü\n",
        "batch_idx, (example_imgs, example_targets) = next(enumerate(mnist_train_dataloader))\n",
        "# info about the dataset\n",
        "D_in = np.prod(example_imgs.shape[1:])\n",
        "D_out = len(mnist_train_dataloader.dataset.targets.unique())\n",
        "print(\"Datasets shapes:\", {x: dataloaders[x].dataset.data.shape for x in ['train', 'val']})\n",
        "print(\"N input features:\", D_in, \"Output classes:\", D_out)\n",
        "print(\"Train batch:\", example_imgs.shape, example_targets.shape)\n",
        "batch_idx, (example_imgs, example_targets) = next(enumerate(mnist_val_dataloader))\n",
        "print(\"Val batch:\", example_imgs.shape, example_targets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnFAmoinjY1T"
      },
      "source": [
        "We can plot some examples with corresponding labels using the following function. This function can also receive the predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "5ZWvjQOvC2ep",
        "outputId": "7abb9b79-83d7-4bfb-d1f4-6e3fdba72359"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_img_label_prediction(imgs, y_true, y_pred=None, shape=(2, 3)):\n",
        "    y_pred = [None] * len(y_true) if y_pred is None else y_pred\n",
        "    fig = plt.figure()\n",
        "    for i in range(np.prod(shape)):\n",
        "        plt.subplot(*shape, i+1)\n",
        "        plt.tight_layout()\n",
        "        plt.imshow(imgs[i][0], cmap='gray', interpolation='none')\n",
        "        plt.title(\"True: {} Pred: {}\".format(y_true[i], y_pred[i]))\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "plot_img_label_prediction(imgs=example_imgs, y_true=example_targets, y_pred=None, shape=(2, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj3utDDuzDCj"
      },
      "source": [
        "## Question 3.1.1\n",
        "### Logistic Regression\n",
        "\n",
        "We can use a very simple Logistic Regression that receives our input images as a vector and predicts the digit. This will be our first baseline to compare with the CNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TniyY4bQzBMS",
        "outputId": "5aa11ea8-84a6-4757-9ac8-7e950fc3f1f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test score with penalty: 0.9035\n"
          ]
        }
      ],
      "source": [
        "# Use standard scaler to transform input data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(np.reshape(X_train, (X_train.shape[0], -1)))\n",
        "X_val = scaler.transform(np.reshape(X_val, (X_val.shape[0], -1)))\n",
        "\n",
        "# Use scikit-learn Logistic Regression with C=50., multi_class='multinomial', solver='sag', tol=0.1\n",
        "\n",
        "clf = LogisticRegression(C=50., multi_class='multinomial', solver='sag', tol=0.1)\n",
        "clf.fit(X_train, y_train)\n",
        "score = clf.score(X_val, y_val)\n",
        "\n",
        "print(\"Test score with penalty: %.4f\" % score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8rylkCnrwIy"
      },
      "source": [
        "We can select the coefficients for each class and reshape them into the image shape to plot them. This allows us to visualize what are the pixels that are contributing more to the classification for each of the digits.\n",
        "\n",
        "But what happens if the digits are not centered? Will we still get such a good performance? Lets test that out later!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK8v34AK6xXJ"
      },
      "source": [
        "## Question 3.1.2\n",
        "### Feed-Forward Neural Network\n",
        "\n",
        "The first step is to create the functions that will allow us to implement a feed-forward neural network and manage the training and validation process.\n",
        "\n",
        "The MLP class will define the architecture of a feed-forward neural network, with a set of hidden layers (fully connected layers [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)), with a activation function in between them ([relu](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu)), and a [softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax) in the last layer. Since the dataset poses a multiclass classification problem, the last layer should have a number of neurons equal to the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "In9r_o8vvNaz"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim_layers):\n",
        "        super(MLP, self).__init__()\n",
        "        self.dim_layers = dim_layers\n",
        "        self.nn = nn.Sequential(nn.Linear(dim_layers[0], dim_layers[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(dim_layers[1], dim_layers[2]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(dim_layers[2], dim_layers[3]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(dim_layers[3], dim_layers[4]),\n",
        "                      nn.LogSoftmax()\n",
        "        )\n",
        "        \n",
        "    def forward(self, X):\n",
        "        # reshape X\n",
        "        X = X.reshape(-1, self.dim_layers[0])\n",
        "        # use softmax for output layer\n",
        "        return self.nn(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6OVD_1xUwWH"
      },
      "source": [
        "##### training validation function for the MLP and CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B1eUu01N8wIR"
      },
      "outputs": [],
      "source": [
        "def train_val_model(model, criterion, optimizer, dataloaders, num_epochs=25,\n",
        "        scheduler=None, log_interval=None):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # init dictionaries to save losses and accuracies of training and validation\n",
        "    losses, accuracies = dict(train=[], val=[]), dict(train=[], val=[])\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        if log_interval is not None and epoch % log_interval == 0:\n",
        "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "            print('-' * 10)\n",
        "\n",
        "        # execute a training and validation phase for each epoch\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # set model to train mode\n",
        "            else:\n",
        "                model.eval()   # Set model to eval mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # iterate over the data\n",
        "            nsamples = 0\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                nsamples += inputs.shape[0]\n",
        "\n",
        "                # set the parameter gradients to zero\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # if in training phase, perform backward prop and optimize\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # increment loss and correct counts\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if scheduler is not None and phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / nsamples\n",
        "            epoch_acc = running_corrects.double() / nsamples\n",
        "\n",
        "            losses[phase].append(epoch_loss)\n",
        "            accuracies[phase].append(epoch_acc)\n",
        "            if log_interval is not None and epoch % log_interval == 0:\n",
        "                print('{} Loss: {:.4f} Acc: {:.2f}%'.format(\n",
        "                    phase, epoch_loss, 100 * epoch_acc))\n",
        "\n",
        "            # deep copy the best model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        if log_interval is not None and epoch % log_interval == 0:\n",
        "            print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:.2f}%'.format(100 * best_acc))\n",
        "\n",
        "    # load best model weights to return\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, losses, accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CBE5tRMZEfr"
      },
      "source": [
        "We will start by creating a simple network with some hidden layers. Thus, in addition to the input, it will have 3 fully connected layer which, in this implemetation, is assigned to the input of the MLP Class. We will use the Stochastic Gradient Descend optimizer ([optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)) with 0.01 learning rate and 0.5 momentum. The loss function to be optimized will be negative log likelihood ([nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)). Training and validation will be managed by the function \"train_val_model\" previously define."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "200WI3xND6_M",
        "outputId": "173c83bc-e0bf-4ed6-b7fc-b44ad318387d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/14\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rodsc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.8439 Acc: 75.89%\n",
            "val Loss: 0.3042 Acc: 90.86%\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.1785 Acc: 94.78%\n",
            "val Loss: 0.1456 Acc: 95.63%\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.1083 Acc: 96.84%\n",
            "val Loss: 0.1162 Acc: 96.41%\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.0750 Acc: 97.77%\n",
            "val Loss: 0.0848 Acc: 97.44%\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.0546 Acc: 98.40%\n",
            "val Loss: 0.0740 Acc: 97.71%\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.0406 Acc: 98.84%\n",
            "val Loss: 0.0730 Acc: 97.65%\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.0294 Acc: 99.19%\n",
            "val Loss: 0.0670 Acc: 97.83%\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.0217 Acc: 99.47%\n",
            "val Loss: 0.0642 Acc: 98.04%\n",
            "\n",
            "Training complete in 2m 31s\n",
            "Best val Acc: 98.09%\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# instantiate model\n",
        "torch.manual_seed(42)\n",
        "model_mlp = MLP([D_in, 256, 128, 64, D_out]).to(device)\n",
        "# instantiate optimizer and criterion\n",
        "optimizer = optim.SGD(model_mlp.parameters(), lr=0.01, momentum=0.5)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# train and validate model\n",
        "model_mlp, losses, accuracies = train_val_model(model_mlp, criterion, optimizer, dataloaders,\n",
        "                       num_epochs=15, log_interval=2)\n",
        "\n",
        "# plot losses\n",
        "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'DataLoader' object is not an iterator",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not an iterator"
          ]
        }
      ],
      "source": [
        "print(dataloaders['train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw-k2B1ldd-i"
      },
      "source": [
        "## Question 3.1.3\n",
        "### Plots of the coefficients of the Logistic Regression for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "2pucfjpaDF9_",
        "outputId": "e21dec94-a4f1-40d6-cf20-d956a53515b0"
      },
      "outputs": [],
      "source": [
        "coef = clf.coef_.copy()\n",
        "plt.figure(figsize=(10, 5))\n",
        "scale = np.abs(coef).max()\n",
        "for i in range(10):\n",
        "    l1_plot = plt.subplot(2, 5, i + 1)\n",
        "    # iterate over the classes and reshape coefficients to ensure proper visualization\n",
        "    l1_plot.imshow(<COMPLETE_HERE>, interpolation='nearest',\n",
        "                   cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n",
        "    l1_plot.set_xticks(())\n",
        "    l1_plot.set_yticks(())\n",
        "    l1_plot.set_xlabel('Class %i' % i)\n",
        "plt.suptitle('Classification coefficient vectors for...')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXhHtX1TkUba"
      },
      "source": [
        "## Question 3.2\n",
        "### Convolutional Neural Network\n",
        "\n",
        "Convolutional layers capture patterns corresponding to relevant features independently of where they occur in the input. To do so, they slide a window over the input and apply the convolution operation with a set of kernels or filters that represent the features. Although it is not their only field of application, convolutional neural networks are mainly praised for their performance on image processing tasks.\n",
        "\n",
        "The training and validation management for the CNN implementation will be performed as the feed-forward network, however we will have to define the network's architecture.\n",
        "\n",
        "For that we will implement a CNN class to define how many layers it comprises and how the layers will be connected.\n",
        "\n",
        "The initialization (`__init__`) function will define the architecture and the `forward` function will implement how the different layers are connected. This architecture will be a sequece of 2 convolutional layers ([nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)) (1st: output channels 10, kernel size 5; 2nd: output channels 20, kernel size 5), then 2 fully connected layers ([nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) (1st: output features 50; 2nd: output features 10 (the number of classes)). Once again, the final layer will be a [softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax) function  that will choose the most probable class of the 10 in the input.\n",
        "\n",
        "Between the second convolution layer and the first fully connected, we will set a dropout layer ([nn.Dropout2d](https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html)). The idea behind dropout is to disable a percentage of randomly selected neurons during each step of the training phase, in order to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ0mCl24EoaM"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"Basic Pytorch CNN for MNIST-like data.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.nn1 = nn.Linear(320, 50)\n",
        "        self.nn2 = nn.Linear(50, 10)\n",
        "        # define convolution and linear layers\n",
        "        \n",
        "        \n",
        "    def forward(self, x, T=1.0):\n",
        "        # Batch size = 64, images 28x28 =>\n",
        "        #     x.shape = [64, 1, 28, 28]\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)),2)\n",
        "        # Convolution with 5x5 filter without padding and 10 channels =>\n",
        "        #     x.shape = [64, 10, 24, 24] since 24 = 28 - 5 + 1\n",
        "        # Max pooling with stride of 2 =>\n",
        "        #     x.shape = [64, 10, 12, 12]\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x),2))\n",
        "        # Convolution with 5x5 filter without padding and 20 channels =>\n",
        "        #     x.shape = [64, 20, 8, 8] since 8 = 12 - 5 + 1\n",
        "        # Max pooling with stride of 2 =>\n",
        "        #     x.shape = [64, 20, 4, 4]\n",
        "        x.view(-1, 320)\n",
        "        # Reshape =>\n",
        "        #     x.shape = [64, 320]\n",
        "        x = F.relu(self.nn1(x))\n",
        "        x = F.softmax(self.nn2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv9vdZZ7OlSh"
      },
      "source": [
        "As previously, lets describe the model to be trained. We will use the ADAM optimizes ([optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)), with learning rate 0.001, and the same negative log likelihood ([nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ImTlr5JeEsb6",
        "outputId": "1cc8475e-39a4-4910-cd3b-878a668047e5"
      },
      "outputs": [],
      "source": [
        "# instatiate the model, optimizer and criterion\n",
        "\n",
        "\n",
        "# train and validate model\n",
        "\n",
        "\n",
        "# plot the losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULZ91b0cPhy5"
      },
      "source": [
        "We have now completed training and validation with 3 different models: Logistic Regression, Feed-Forward Network, and Convolutional Neural Network.\n",
        "\n",
        "We have seen that with the CNN, the performance of the model in the validation set, outperforms the other models (~99% accuracy against ~90% and ~98%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al8Axr0sdd-n"
      },
      "source": [
        "## Question 3.3\n",
        "### Number of learnable parameters of the MLP and the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHyGUuZbTvhr"
      },
      "source": [
        "The difference in performance between CNNs and MLP is small but how many learnable parameters are we using in the MLP and in CNN models?\n",
        "\n",
        "We can find it out using the following lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acy0l3-YQjT2",
        "outputId": "3ba26bee-5fa5-42ff-ea44-91e242d99f1a"
      },
      "outputs": [],
      "source": [
        "# get MLP trainable model parameters\n",
        "\n",
        "print('Number of parameters in the MLP model: {}'.format(params_mlp))\n",
        "\n",
        "# get CNN trainable model parameters\n",
        "\n",
        "print('Number of parameters in the CNN model: {}'.format(params_cnn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj28CWvrMbOw"
      },
      "source": [
        "You can see that we have ~11x more learnable parameters to achieve almost the same performance.\n",
        "\n",
        "We can experiment and try to find out the number of layers and corresponding sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKB4FCyodd-p"
      },
      "source": [
        "## Question 3.4\n",
        "### Change number of MLP hidden layers to approximate the number of learnable parameters of the CNN. What is the impact on the performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RmgJhIPMECw",
        "outputId": "cbc4b2c9-7132-46f5-9b06-8bc62af38db2"
      },
      "outputs": [],
      "source": [
        "# instantiate smaller MLP\n",
        "\n",
        "print('Number of parameters in the MLP model: {}'.format(params_mlp_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_oq9682QWCF"
      },
      "source": [
        "And how does that model perform? We are about to find out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "w6oa0TeBQU9E",
        "outputId": "4e0844b7-5fe0-4b96-daa4-3762fdfc1f35"
      },
      "outputs": [],
      "source": [
        "#instantiate optimizer and criterion\n",
        "\n",
        "\n",
        "# train model\n",
        "\n",
        "\n",
        "#plot losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpmgachOUCnX"
      },
      "source": [
        "We can see a drop in performance compared with the previous MLP model. So we can understand that although we have less learnable parameters, due to properties of CNNs (e.g., invariance and parameter sharing), which allow them to have fewer weights as some parameters are shared.\n",
        "\n",
        "## Question 3.5\n",
        "### What happens in each of the classifiers if for some reason your validation set does not always have the digits centered in your input image?\n",
        "\n",
        "CNNs are expected to be invariant to the location where important features occur in the input. In fact, it's not unusual that there is a dataset shift where the data acquisition process suffers some modification. We will do this by applying a transformation with horizontal translations to our validation dataset and see how robust each model is to these shifts.\n",
        "\n",
        "We can do this by going back to **0.1 - Create Dataloaders -\n",
        "MNIST dataset** cell to define the test transform using the following code\n",
        "\n",
        "```\n",
        "mnist_transform_val = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.RandomAffine(0, translate=[0.1, 0]),\n",
        "     transforms.Normalize((0.1307,), (0.3081,))])\n",
        "```\n",
        "\n",
        "and replace\n",
        "\n",
        "`mnist_val_dataset = datasets.MNIST('../data', download=True, train=False, transform=mnist_transform)`\n",
        "\n",
        "with\n",
        "\n",
        "`mnist_val_dataset = datasets.MNIST('../data', download=True, train=False, transform=mnist_transform_val)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FH6naktdd-r"
      },
      "outputs": [],
      "source": [
        "# These random translations will be added in the end of this notebook, for now we skip this.\n",
        "mnist_transform_val = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.RandomAffine(0, translate=[0.1, 0]),\n",
        "     transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# Create MNIST validation dataloaders using transform validation with translation\n",
        "mnist_val_dataset = datasets.MNIST('../data', download=True, train=False, transform=mnist_transform_val)\n",
        "mnist_val_dataloader = DataLoader(mnist_val_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# MNIST Dataloader for to get data into numpy for Logistic Regression\n",
        "mnist_val_dataloader_numpy = DataLoader(mnist_val_dataset, batch_size=len(mnist_val_dataset))\n",
        "X_y_val = next(iter(mnist_val_dataloader_numpy))\n",
        "X_val = X_y_val[0].numpy()\n",
        "y_val = X_y_val[1].numpy()\n",
        "\n",
        "dataloaders = dict(train=mnist_train_dataloader, val=mnist_val_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VahBzmmedd-s"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fe3K78qdd-s",
        "outputId": "ce719bf2-3d63-4ac6-fb6b-c93f0bdd8fd0"
      },
      "outputs": [],
      "source": [
        "# score the model on the validation dataset after translation\n",
        "\n",
        "print(\"Test score with penalty: %.4f\" % score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKXir18Udd-t"
      },
      "source": [
        "#### MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vehhij_Odd-t",
        "outputId": "d1318f43-d74f-48f3-f8fc-5c1fe9eb4643"
      },
      "outputs": [],
      "source": [
        "#instantiate the model\n",
        "\n",
        "\n",
        "#instantiate optimizer and criterion\n",
        "\n",
        "\n",
        "#train model and obtain loss and accuracy for translated validation dataset\n",
        "\n",
        "\n",
        "# plot losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dxwYacqdd-u"
      },
      "source": [
        "#### CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OFq2OFmydd-u",
        "outputId": "ce6751bd-6403-4583-ba92-f5c2bd59c8ba"
      },
      "outputs": [],
      "source": [
        "# instantiate model, optimizer and criterion\n",
        "\n",
        "\n",
        "# train model and obtain loss and accuracy for the translated validation dataset\n",
        "\n",
        "\n",
        "# plot losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5gcf_gMlcqI"
      },
      "source": [
        "After rerunning the different models we can see that the accuracy of the Logistic Regression drops from ~90% to ~72%, the MLP drops from ~98% to ~87%, and the CNN drops from ~99% to ~97%. This shows that the learned features are more robust to variances in location, as expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU3NwQ7Nuvhv"
      },
      "source": [
        "# Bonus Case - Attention with small images and CNNs.  (And how to create a dataset that takes numpy arrays)\n",
        "\n",
        "In this case we will use the Scikit-Learn's digits dataset\n",
        "\n",
        "## Scikit-Learn Digits\n",
        "\n",
        "This dataset is provided by scikit-learn and the digit images are returned as numpy ndarray. We will use PIL (Python Image Library) to convert the numpy ndarray to a image, tranform it to a tensor and normalize it.\n",
        "\n",
        "In this case we don't have a predefined Digits Dataset provided by torchvision so we will need to write a custom Dataset class and implement three functions:\n",
        "\n",
        "`__init__`, `__len__`, and `__getitem__`.\n",
        "\n",
        "Scikit-Learn return the digits images and labels as ndarrays. Each digit image is an 8x8 array.\n",
        "\n",
        "To use the previous CNN, we will use a transform to resize the images to the MNIST image size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4v-XFzcv9If"
      },
      "outputs": [],
      "source": [
        "SKLEARN_DIGITS_TRAIN_SIZE = 1247\n",
        "SKLEARN_DIGITS_VAL_SIZE = 550\n",
        "\n",
        "class NumpyDataset(Dataset):\n",
        "\n",
        "  def __init__(self, data, targets, transform=None):\n",
        "    self.data = torch.from_numpy(data).float()\n",
        "    self.targets = torch.from_numpy(targets).long()\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = np.expand_dims(self.data[index], axis=2)\n",
        "    y = self.targets[index]\n",
        "    if self.transform:\n",
        "        x = self.transform(x)\n",
        "    return x, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "digits_transform = transforms.Compose([\n",
        "                                       transforms.ToPILImage(),\n",
        "                                       transforms.Resize(28),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       ])\n",
        "\n",
        "# Get sklearn digits dataset\n",
        "X, y = load_digits(return_X_y=True)\n",
        "X = X.reshape((len(X), 8, 8))\n",
        "y_train = y[:-SKLEARN_DIGITS_VAL_SIZE]\n",
        "y_val = y[-SKLEARN_DIGITS_VAL_SIZE:]\n",
        "X_train = X[:-SKLEARN_DIGITS_VAL_SIZE]\n",
        "X_val = X[-SKLEARN_DIGITS_VAL_SIZE:]\n",
        "\n",
        "digits_train_dataset = NumpyDataset(X_train, y_train, transform=digits_transform)\n",
        "digits_val_dataset = NumpyDataset(X_val, y_val, transform=digits_transform)\n",
        "digits_train_dataloader = torch.utils.data.DataLoader(digits_train_dataset, batch_size=64, shuffle=True)\n",
        "digits_val_dataloader = torch.utils.data.DataLoader(digits_val_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "dataloaders = dict(train=digits_train_dataloader, val=digits_val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhQU3v7Zv9Ih"
      },
      "outputs": [],
      "source": [
        "# Get some examples of images and targets\n",
        "_, (example_train_imgs, example_train_targets) = next(enumerate(digits_train_dataloader))\n",
        "_, (example_val_imgs, example_val_targets) = next(enumerate(digits_val_dataloader))\n",
        "\n",
        "# Info about the dataset\n",
        "D_in = np.prod(example_imgs.shape[1:])\n",
        "D_out = len(digits_train_dataloader.dataset.targets.unique())\n",
        "\n",
        "# Output information\n",
        "print(\"Datasets shapes (before transformations):\", {x: dataloaders[x].dataset.data.shape for x in ['train', 'val']})\n",
        "print(\"N input features:\", D_in, \"Output classes:\", D_out)\n",
        "print(\"Train batch:\", example_train_imgs.shape, example_train_targets.shape)\n",
        "print(\"Val batch:\", example_val_imgs.shape, example_val_targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx78pb7Ov9Ih"
      },
      "outputs": [],
      "source": [
        "plot_img_label_prediction(imgs=example_train_imgs, y_true=example_train_targets, y_pred=None, shape=(2, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbBAH9OTv9Ii"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W46ofUE6v9Ii"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "print(X_train.squeeze().shape)\n",
        "X_train = scaler.fit_transform(np.reshape(X_train, (X_train.shape[0], -1)))\n",
        "X_val = scaler.transform(np.reshape(X_val, (X_val.shape[0], -1)))\n",
        "\n",
        "# Turn up tolerance for faster convergence\n",
        "clf = LogisticRegression(C=50., multi_class='multinomial', solver='sag', tol=0.1)\n",
        "clf.fit(X_train, y_train)\n",
        "#sparsity = np.mean(clf.coef_ == 0) * 100\n",
        "score = clf.score(X_val, y_val)\n",
        "\n",
        "print(\"Test score with penalty: %.4f\" % score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_G2Rt0Gv9Ij"
      },
      "outputs": [],
      "source": [
        "coef = clf.coef_.copy()\n",
        "plt.figure(figsize=(10, 5))\n",
        "scale = np.abs(coef).max()\n",
        "for i in range(10):\n",
        "    l1_plot = plt.subplot(2, 5, i + 1)\n",
        "    l1_plot.imshow(coef[i].reshape(8, 8), interpolation='nearest',\n",
        "                   cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n",
        "    l1_plot.set_xticks(())\n",
        "    l1_plot.set_yticks(())\n",
        "    l1_plot.set_xlabel('Class %i' % i)\n",
        "plt.suptitle('Classification coefficient vectors for...')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfYdXpde4bg0"
      },
      "source": [
        "### Feed-forward using digits dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDKn4WS636Bg"
      },
      "outputs": [],
      "source": [
        "model = MLP([D_in, 512, 256, 128, 64, D_out]).to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "model, losses, accuracies = train_val_model(model, criterion, optimizer, dataloaders,\n",
        "                       num_epochs=20, log_interval=2)\n",
        "\n",
        "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9ImEBeo4MW6"
      },
      "outputs": [],
      "source": [
        "model = CNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "model, losses, accuracies = train_val_model(model, criterion, optimizer, dataloaders,\n",
        "                       num_epochs=50, log_interval=10)\n",
        "\n",
        "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIhei09Ruvf-"
      },
      "source": [
        "# Bonus Information - Visualizing CNN filters\n",
        "\n",
        "Some work have been done to demonstrate the type of features learned by different filters in different layers.\n",
        "\n",
        "For instance, considering a known CNN called **VGG16** which has the following architecture\n",
        "\n",
        "![image](https://media.geeksforgeeks.org/wp-content/uploads/20200219152327/conv-layers-vgg16.jpg)\\[taken from: https://www.geeksforgeeks.org/vgg-16-cnn-model/ \\]\n",
        "\n",
        "these would be some of the filters from some of the layers:\n",
        "<table border=0 width=\"500px\" >\n",
        "\t<tbody>\n",
        "\t\t<tr>\n",
        "\t\t\t<td width=\"19%\" align=\"center\"> Layer 2 <br /> (Conv 1-2)</td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l2_f1.jpg\"> </td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l2_f21.jpg\"> </td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l2_f54.jpg\"> </td>\n",
        "\t\t</tr>\n",
        "\t\t<tr>\n",
        "\t\t\t<td width=\"19%\" align=\"center\"> Layer 10 <br /> (Conv 2-1)</td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l10_f7.jpg\"> </td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l10_f10.jpg\"> </td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l10_f69.jpg\"> </td>\n",
        "\t\t</tr>\n",
        "\t\t<tr>\n",
        "\t\t\t<td width=\"19%\" align=\"center\"> Layer 17 <br /> (Conv 3-1)</td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l17_f4.jpg\"> </td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l17_f8.jpg\"> </td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l17_f9.jpg\"> </td>\n",
        "\t\t</tr>\n",
        "\t\t<tr>\n",
        "\t\t\t<td width=\"19%\" align=\"center\"> Layer 24 <br /> (Conv 4-1)</td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l24_f4.jpg\"> </td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l24_f17.jpg\"> </td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/results/layer_visualizations/layer_vis_l24_f22.jpg\"> </td>\n",
        "\t\t</tr>\n",
        "\t</tbody>\n",
        "</table>\n",
        "\n",
        "or obtain the class activations:\n",
        "<table border=0 width=\"500px\" >\n",
        "\t<tbody>\n",
        "    <tr>\t\t<td width=\"27%\" align=\"center\"> Input Image </td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> Layer Vis. (Filter=0)</td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> Filter Vis. (Layer=29)</td>\n",
        "\t\t</tr>\n",
        "<tr>\n",
        "\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/pytorch-cnn-visualizations/master/input_images/spider.png\"> </td>\n",
        "\t\t\t<td width=\"27%\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/spider_layer_graph.gif\"> </td>\n",
        "\t\t\t<td width=\"27%\" align=\"center\"> <img src=\"https://raw.githubusercontent.com/utkuozbulak/cnn-gifs/master/spider_filter_graph.gif\"> </td>\n",
        "\t\t</tr>\n",
        "\t</tbody>\n",
        "</table>\n",
        "\n",
        "\\[examples taken from: http://www.github.com/utkuozbulak/pytorch-cnn-visualizations \\]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU21L86BvAXK"
      },
      "source": [
        "# Bonus Information - Predefined architectures, pre-trained models and transfer learning\n",
        "\n",
        "Packages like [torchvision](https://pytorch.org/vision/stable/index.html) and [timm](https://rwightman.github.io/pytorch-image-models/) offer you the possibility of using predefined architectures or even use pre-trained models that can be used to fine tune the models for that same task or used for transfer learning.\n",
        "\n",
        "Besides datasets, transforms and others, **Torchvision** has a large number of predefined architecture with the possibility of loading the pre-trained weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_1YpmkV-PbU"
      },
      "source": [
        "#### Torchvision classification models examples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5yOWA9l4lrF"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "# construct a model with random weights to be trained\n",
        "resnet18 = models.resnet18()\n",
        "\n",
        "# load a pre-trained model\n",
        "resnet18 = models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAsfqUIyDBD7"
      },
      "source": [
        "For examples of different models and how to use pre-trained weights please visit https://pytorch.org/vision/stable/models.html#\n",
        "\n",
        "\n",
        "\n",
        "Another possibility is **timm** which contains models for classification only.\n",
        "In **timm** you are not restricted to have inputs only with 1/3-channels, allowing you to use architectures or pre-trained models using images that have 2 or > 3-channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zS8Ykbo-ZUg"
      },
      "source": [
        "#### timm classification models examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0FQYnKQ-lr_"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  !pip install -q timm\n",
        "import timm\n",
        "\n",
        "# list all models\n",
        "print(timm.list_models())\n",
        "\n",
        "# list pre-trained models\n",
        "print(timm.list_models(pretrained=True))\n",
        "\n",
        "# list models architectures by wildcards\n",
        "print(timm.list_models('*resne*t*'))\n",
        "\n",
        "# construct a model with random weights to be trained\n",
        "model = timm.create_model('resnet18')\n",
        "\n",
        "# load a pre-trained model\n",
        "model = timm.create_model('resnet18', pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSEwqB8ADIao"
      },
      "source": [
        "For more details on how to use this package visit https://rwightman.github.io/pytorch-image-models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaank-9kDI72"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
